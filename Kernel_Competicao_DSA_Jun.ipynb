{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competição DSA de Machine Learning - Edição Junho/2019\n",
    "Imagine estar com fome em uma parte desconhecida da cidade e receber recomendações de restaurantes, com base em suas preferências pessoais, no momento certo. A recomendação vem com um desconto em anexo da sua operadora de cartão de crédito para um local ao virar a esquina!\n",
    "\n",
    "Uma Startup pensou nisso e construiu parcerias com comerciantes para oferecer promoções ou descontos aos portadores de cartões de crédito. Mas essas promoções funcionam tanto para o consumidor quanto para o comerciante? Os clientes aproveitam a experiência? Os comerciantes veem resultado? A personalização é fundamental.\n",
    "\n",
    "Os profissionais da Startup construíram modelos de aprendizado de máquina para entender os aspectos e preferências mais importantes no ciclo de vida de seus clientes, desde alimentos a compras. Mas até agora nenhum deles é especificamente adaptado para um indivíduo ou perfil. É aqui que você entra. Precisando de um modelo preditivo mais robusto, a Startup selecionou você como Cientista de Dados.\n",
    "\n",
    "\n",
    "## The Goal\n",
    " \n",
    "- Os arquivos dataset_treino.csv e dataset_teste.csv contêm card_ids e informações sobre o próprio cartão - o primeiro mês em que o cartão estava ativo, etc.\n",
    "- Objetivo é prever um índice de lealdade para cada card_id\n",
    "- O modelo é avalidado pelo Root-Mean-Squared-Error (RMSE)\n",
    "\n",
    "## Key features of the model training process in this kernel:\n",
    "- **Cross Validation:** Using 2-fold cross-validation (para testar mais rapido)\n",
    "- **Models:** svr, gradient boosting, random forest, xgboost, lightgbm e keras regressors\n",
    "- **Blending:** Para ter as previsoes finais eu juntei os modelos para obter uma performance melhor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo e Carregando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas que serao utilizadas neste projeto\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.svm import SVR\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "\n",
    "# Stats\n",
    "from scipy.stats import skew, norm\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "# Misc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Ignore useless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "pd.options.display.max_seq_items = 8000\n",
    "pd.options.display.max_rows = 8000\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import gc \n",
    "import pickle\n",
    "import datetime\n",
    "import os\n",
    "print(os.listdir(\"data\"))\n",
    "#print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'../input/datadsa/transacoes_historicas/transacoes_historicas.csv'\n",
    "hist = pd.read_csv('data/transacoes_historicas.csv'\n",
    "                            ,parse_dates=['purchase_date']\n",
    "                            ,dtype = {\n",
    "                                'city_id': np.int16\n",
    "                                ,'installments': np.int16\n",
    "                                ,'merchant_category_id': np.int16\n",
    "                                ,'month_lag': np.int8\n",
    "                                ,'purchase_amount': np.float32\n",
    "                                ,'state_id': np.int8\n",
    "                                ,'subsector_id': np.int8\n",
    "                            }) \n",
    "\n",
    "#''../input/competicao-dsa-machine-learning-jun-2019/novas_transacoes_comerciantes.csv\n",
    "novas = pd.read_csv('data/novas_transacoes_comerciantes.csv'\n",
    "                            ,parse_dates=['purchase_date']\n",
    "                            ,dtype = {\n",
    "                                'city_id': np.int16\n",
    "                                ,'installments': np.int16\n",
    "                                ,'merchant_category_id': np.int16\n",
    "                                ,'month_lag': np.int8\n",
    "                                ,'purchase_amount': np.float32\n",
    "                                ,'state_id': np.int8\n",
    "                                ,'subsector_id': np.int8\n",
    "                            })   \n",
    "\n",
    "#''../input/competicao-dsa-machine-learning-jun-2019/dataset_treino.csv\n",
    "train = pd.read_csv('data/dataset_treino.csv'\n",
    "                       ,parse_dates=['first_active_month']\n",
    "                       ,dtype = {\n",
    "                                'feature_1': np.int8\n",
    "                                ,'feature_2': np.int8\n",
    "                                ,'feature_3': np.int8\n",
    "                            })\n",
    "\n",
    "#'../input/competicao-dsa-machine-learning-jun-2019/comerciantes.csv'\n",
    "com = pd.read_csv('data/comerciantes.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset as a dataframe\n",
    "test = pd.read_csv('data/dataset_teste.csv'\n",
    "                        ,parse_dates=['first_active_month']\n",
    "                        ,dtype = {\n",
    "                                'feature_1': np.int8\n",
    "                               ,'feature_2': np.int8\n",
    "                               ,'feature_3': np.int8\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um index para o dataframe de treino\n",
    "train = train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.concat( [hist, novas],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novas.shape, hist.shape, tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniao dos dataset de treino com transacoes novas e historicas\n",
    "df = pd.merge(train, tmp, on='card_id', how='left')\n",
    "\n",
    "# Uniao dos dataset de treino e teste com comerciantes\n",
    "df = pd.merge(df, com, on='merchant_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniao dos dataset de teste com transacoes novas e historicas\n",
    "dfTest = pd.merge(test, tmp, on='card_id', how='left')\n",
    "\n",
    "# Uniao dos dataset de teste e teste com comerciantes\n",
    "dfTest = pd.merge(dfTest, com, on='merchant_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, hist, tmp, novas, com\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removendo algumas colunas do modelo repetidas e outras que nao afetaram o resultado (pelo menos ate agora)\n",
    "Pode ser que criando novas features com essas colunas seja interessante para melhorar a performance do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = [\"merchant_category_id_y\",\n",
    "                   \"subsector_id_y\", \n",
    "                   \"city_id_y\", \n",
    "                   \"state_id_y\", \n",
    "                   \"category_1_y\", \n",
    "                   \"category_2_y\",\n",
    "                   \"category_3\", \n",
    "                   \"merchant_id\"\n",
    "                  ], inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest.drop(columns = [\"merchant_category_id_y\",\n",
    "                   \"subsector_id_y\", \n",
    "                   \"city_id_y\", \n",
    "                   \"state_id_y\", \n",
    "                   \"category_1_y\", \n",
    "                   \"category_2_y\",\n",
    "                   \"category_3\", \n",
    "                   \"merchant_id\"\n",
    "                  ], inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparacao estatistica entre os dois datasets (treino e teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas observacoes daqui:\n",
    "\n",
    "1. alguns valores infinito e NaN nas colunas avg_purchases_lag3, avg_purchases_lag6 e avg_purchases_lag12 em ambos os dataset (treino e teste)\n",
    "2. a media dos dados estao relativamente proximos entre os dataset de treino e teste\n",
    "3. o valor medio parece estar relativamente em um range pequeno\n",
    "4. a feature numerical_1 e numerical_2 sao muito parecidas, pouca variacao\n",
    "5. a media de valores da coluna avg_sales_lag6 entre os dois modelos é bem diferente (vale a pena verificar se impacta na performance do modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando valores missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_missing(df):\n",
    "    data = pd.DataFrame(df)\n",
    "    df_cols = list(pd.DataFrame(data))\n",
    "    dict_x = {}\n",
    "    for i in range(0, len(df_cols)):\n",
    "        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n",
    "    \n",
    "    return dict_x\n",
    "\n",
    "missing = percent_missing(df)\n",
    "df_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\n",
    "print('Percent of missing data')\n",
    "df_miss[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup do plot\n",
    "sns.set_style(\"white\")\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "sns.set_color_codes(palette='deep')\n",
    "\n",
    "# Identificando os valores missing\n",
    "missing = round(df.isnull().mean()*100,2)\n",
    "missing = missing[missing > 0]\n",
    "missing.sort_values(inplace=True)\n",
    "missing.plot.bar(color=\"b\")\n",
    "\n",
    "# Visual presentation\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Percent of missing values\")\n",
    "ax.set(xlabel=\"Features\")\n",
    "ax.set(title=\"Percent missing data by feature\")\n",
    "sns.despine(trim=True, left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos substituir os valores INF por NaN e preencher os valores NaN por 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df.update(df.fillna(df.median()))\n",
    "#df.update(df.fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos fazer uma copia do dataset de Treino para facilitar e acelerar algumas analises e acuracia do modelo. A principio usarei uma amostra de 100.000 registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample = df\n",
    "dfSample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise Exploratória de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando os tipos de dados do dataset\n",
    "dfSample.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos transformar algumas variaveis categoricas em numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando algumas variaveis string para inteiro\n",
    "cleanup_nums = {\"authorized_flag\":             {\"N\": 0, \"Y\": 1},\n",
    "                \"category_1_x\":                {\"N\": 0, \"Y\": 1},\n",
    "                \"category_3\":                  {\"A\": 1, \"B\": 2, \"C\": 3},\n",
    "                \"category_4\":                  {\"N\": 0, \"Y\": 1},\n",
    "                \"most_recent_sales_range\":     {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5},\n",
    "                \"most_recent_purchases_range\": {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5}}\n",
    "\n",
    "dfSample.replace(cleanup_nums, inplace=True)\n",
    "dfSample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando algumas variaveis string para inteiro\n",
    "dfTest.replace(cleanup_nums, inplace=True)\n",
    "dfTest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slipt de algumas features do dataset de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de Treino\n",
    "dfSample['first_active_month'] = pd.to_datetime(dfSample['first_active_month'])\n",
    "dfSample['active_dayofweek'] = dfSample.first_active_month.apply(lambda dt: dt.dayofweek)\n",
    "dfSample['active_year'] = dfSample.first_active_month.apply(lambda dt: dt.year)\n",
    "dfSample['active_month'] = dfSample.first_active_month.apply(lambda dt: dt.month)\n",
    "dfSample.drop(columns =[\"first_active_month\"], inplace = True) \n",
    "\n",
    "# Codigo abaixo comentado pois nao funcionou no Kernel (somente na maquina local)\n",
    "dfSample['purchase_date'] = pd.to_datetime(dfSample['purchase_date'])\n",
    "dfSample['purchase_date_day'] = dfSample.purchase_date.apply(lambda dt: dt.day)\n",
    "dfSample['purchase_date_dayofweek'] = dfSample.purchase_date.apply(lambda dt: dt.dayofweek)\n",
    "dfSample['purchase_date_month'] = dfSample.purchase_date.apply(lambda dt: dt.month)\n",
    "dfSample['purchase_date_year'] = dfSample.purchase_date.apply(lambda dt: dt.year)\n",
    "dfSample['purchase_date_hour'] = dfSample.purchase_date.apply(lambda dt: dt.hour)\n",
    "dfSample.drop(columns =[\"purchase_date\"], inplace = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de Test\n",
    "dfTest['first_active_month'] = pd.to_datetime(dfTest['first_active_month'])\n",
    "dfTest['active_dayofweek'] = dfTest.first_active_month.apply(lambda dt: dt.dayofweek)\n",
    "dfTest['active_year'] = dfTest.first_active_month.apply(lambda dt: dt.year)\n",
    "dfTest['active_month'] = dfTest.first_active_month.apply(lambda dt: dt.month)\n",
    "dfTest.drop(columns =[\"first_active_month\"], inplace = True) \n",
    "\n",
    "# Codigo abaixo comentado pois nao funcionou no Kernel (somente na maquina local)\n",
    "dfTest['purchase_date'] = pd.to_datetime(dfTest['purchase_date'])\n",
    "dfTest['purchase_date_day'] = dfTest.purchase_date.apply(lambda dt: dt.day)\n",
    "dfTest['purchase_date_dayofweek'] = dfTest.purchase_date.apply(lambda dt: dt.dayofweek)\n",
    "dfTest['purchase_date_month'] = dfTest.purchase_date.apply(lambda dt: dt.month)\n",
    "dfTest['purchase_date_year'] = dfTest.purchase_date.apply(lambda dt: dt.year)\n",
    "dfTest['purchase_date_hour'] = dfTest.purchase_date.apply(lambda dt: dt.hour)\n",
    "dfTest.drop(columns =[\"purchase_date\"], inplace = True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos plotar um scatter plot para verificar os dados de treino. Vamos visualizar 5% dos dados. Na eixo x vamos colocar as features e no eixo y a variavel target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_scatter(df1, df2, features):\n",
    "    i = 0\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(5,6,figsize=(24,24))\n",
    "\n",
    "    for feature in features:\n",
    "        i += 1\n",
    "        plt.subplot(5,6,i)\n",
    "        plt.scatter(df2[feature], df1['target'], marker='+')\n",
    "        plt.xlabel(feature, fontsize=10)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['feature_1', 'feature_2','feature_3','month_lag', 'purchase_amount', \n",
    "            'avg_sales_lag3', 'avg_sales_lag6', 'avg_sales_lag12', 'avg_purchases_lag3', 'avg_purchases_lag6',\n",
    "            'avg_purchases_lag12','active_months_lag3', 'active_months_lag6', 'active_months_lag12', \n",
    "            'active_dayofweek','active_month','active_year',\n",
    "            'merchant_category_id_x', 'subsector_id_x', 'city_id_x', 'state_id_x',\n",
    "            'category_1_x', 'category_2_x', 'authorized_flag', 'installments',\n",
    "            'merchant_group_id', 'numerical_1', 'numerical_2'\n",
    "           ]\n",
    "plot_feature_scatter(dfSample,dfSample, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando caracteristicas da variável target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette='deep')\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "# Fit a normal distribution\n",
    "mu, std = norm.fit(dfSample['target'])\n",
    "\n",
    "# Verificando a distribuicao de frequencia da variavel TARGET\n",
    "sns.distplot(dfSample['target'], color=\"b\", fit = stats.norm);\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(xlabel=\"target\")\n",
    "ax.set(title=\"Target distribution: mu = %.2f,  std = %.2f\" % (mu, std))\n",
    "sns.despine(trim=True, left=True)\n",
    "\n",
    "# Skewness: It is the degree of distortion from the symmetrical bell curve or the normal distribution\n",
    "# If the skewness is between -0.5 and 0.5, the data are fairly symmetrical.\n",
    "# If the skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed), the data are moderately skewed.\n",
    "# If the skewness is less than -1(negatively skewed) or greater than 1(positively skewed), the data are highly skewed.\n",
    "\n",
    "# Kurtosis: It is actually the measure of outliers present in the distribution.\n",
    "# High kurtosis in a data set is an indicator that data has heavy tails or outliers. \n",
    "# Low kurtosis in a data set is an indicator that data has light tails or lack of outliers\n",
    "\n",
    "ax.text(x=1.1, y=1, transform=ax.transAxes, s=\"Skewness: %f\" % dfSample['target'].skew(),\\\n",
    "        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n",
    "        backgroundcolor='white', color='xkcd:poo brown')\n",
    "ax.text(x=1.1, y=0.95, transform=ax.transAxes, s=\"Kurtosis: %f\" % dfSample['target'].kurt(),\\\n",
    "        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n",
    "        backgroundcolor='white', color='xkcd:dried blood')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir desses graficos com features relacionadas a variavel target é possível perceber os outliers. Entre -20 e -30 nao tem valores... mas -33 tem alguns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot de como as features se correlacionam com cada uma e com a variavel target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample[dfSample.columns.drop('target')].corrwith(dfSample.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poucas variaveis se correlacionam fortemente com a target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize = (30,30))\n",
    "sns.set(font_scale=1.5)\n",
    "sns.heatmap(dfSample.corr(),square = True,cbar=True,annot=True,annot_kws={'size': 10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas observacoes daqui:\n",
    "\n",
    "1. feature_1 e feature_3 tem um forte relacionamento positivo\n",
    "2. avg_sales_lag3 e avg_purchases_lag3 tem um relacionamento praticamente 1 (talvez seja necessario retirar uma dessas variaveis ou juntar as duas por uma multiplicacao, totalizando vendas x compras)\n",
    "3. o mesmo ocorre com as outras variaveis _lag6 e _lag12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar um boxplot de todas as variaveis numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all numeric features\n",
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numeric = []\n",
    "for i in dfSample.columns:\n",
    "    if dfSample[i].dtype in numeric_dtypes:\n",
    "        numeric.append(i)\n",
    "        \n",
    "        \n",
    "# Create box plots for all numeric features\n",
    "sns.set_style(\"white\")\n",
    "f, ax = plt.subplots(figsize=(14, 11))\n",
    "ax.set_xscale(\"log\")\n",
    "ax = sns.boxplot(data=dfSample[numeric] , orient=\"h\", palette=\"Set1\")\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Feature names\")\n",
    "ax.set(xlabel=\"Numeric values\")\n",
    "ax.set(title=\"Numeric Distribution of Features\")\n",
    "sns.despine(trim=True, left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideracoes sobre esse BoxPlot:\n",
    "\n",
    "1. as features avg_sales_lagx e avg_purchase_lagx tem muitos outliers (talvez seja necessario trata-los dependendo do modelo preditivo)\n",
    "2. tambem é necessário verificar a feature 'feature_3'pois exibe um comportamento diferente das demais features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando novamente a distribuição da variavel Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup do plot\n",
    "sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette='deep')\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "# Verificando a distribuicao\n",
    "sns.distplot(dfSample['target'], color=\"b\");\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(xlabel=\"Target\")\n",
    "ax.set(title=\"Target distribution\")\n",
    "sns.despine(trim=True, left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como verificado na analise exploratoria, temos alguns outliers que serao tratados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando mais de perto a variavel target\n",
    "dfSample['target'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O valor minimo é -33.219281 e o valor maximo é de 15.994455, gerando um desvio padrão muito alto em relação a media\n",
    "Neste primeiro momento vou remover estes outiliers diretamente do dataset, para fazer alguns testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo outliers da variavei target (abaixo de -10 e acima de 10)\n",
    "dfSample.drop(dfSample[(dfSample['target'] < -10)].index, inplace=True)\n",
    "dfSample.drop(dfSample[(dfSample['target'] > 10)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale a pena realizar uma transformacao logaritma, para auxiliar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando uma transformacao logaritma\n",
    "# log(1+x) transform\n",
    "dfSample[\"target\"] = np.log1p(dfSample[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois da transformacao, varios valores inf apareceram, por isso vamos preencher com zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample = dfSample.replace([np.inf, -np.inf], np.nan)\n",
    "dfSample.update(dfSample[\"target\"].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette='deep')\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "# Fit a normal distribution\n",
    "mu, std = norm.fit(dfSample['target'])\n",
    "\n",
    "# Verificando a distribuicao de frequencia da variavel TARGET\n",
    "sns.distplot(dfSample['target'], color=\"b\", fit = stats.norm);\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(xlabel=\"target\")\n",
    "ax.set(title=\"Target distribution: mu = %.2f,  std = %.2f\" % (mu, std))\n",
    "sns.despine(trim=True, left=True)\n",
    "\n",
    "ax.text(x=1.1, y=1, transform=ax.transAxes, s=\"Skewness: %f\" % dfSample['target'].skew(),\\\n",
    "        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n",
    "        backgroundcolor='white', color='xkcd:poo brown')\n",
    "ax.text(x=1.1, y=0.95, transform=ax.transAxes, s=\"Kurtosis: %f\" % dfSample['target'].kurt(),\\\n",
    "        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n",
    "        backgroundcolor='white', color='xkcd:dried blood')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variavel target ficou bem proximo de uma distribuicao normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando a distribuicao de outras variaveis com outliers detectado na analise exploratoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup do plot\n",
    "sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette='deep')\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "# Verificando a distribuicao\n",
    "sns.distplot(dfSample['purchase_amount'], color=\"b\");\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(xlabel=\"purchase_amount\")\n",
    "ax.set(title=\"purchase_amount distribution\")\n",
    "sns.despine(trim=True, left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample['purchase_amount'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature 'purchase_amount' tem um desvio padrao muito alto. Para valores abaixo de zero, vamos atribuir 0.01 e para valores acima de 1 vamos atribuir 1 (penso que é o montante de compra, estranho estar negativo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample['purchase_amount'] = dfSample['purchase_amount'].apply(lambda x: 0.01 if x <= 0 else x)\n",
    "dfSample['purchase_amount'] = dfSample['purchase_amount'].apply(lambda x: 1 if x > 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup do plot\n",
    "sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette='deep')\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "# Verificando a distribuicao\n",
    "sns.distplot(dfSample['purchase_amount'], color=\"b\");\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(xlabel=\"purchase_amount\")\n",
    "ax.set(title=\"purchase_amount distribution\")\n",
    "sns.despine(trim=True, left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora dar uma olhada da distribuicao de frequencia das features avg_sales_lagx e avg_purchase_lagx, pois apresentaram muitos outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_dist(df1, features):\n",
    "    i = 0\n",
    "    sns.set_style('whitegrid')\n",
    "    sns.set_color_codes(palette='deep')\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(3,2,figsize=(24,12))\n",
    "\n",
    "    for feature in features:\n",
    "        i += 1\n",
    "        plt.subplot(3,2,i)\n",
    "        sns.distplot(df1[feature], color=\"b\");\n",
    "        plt.xlabel(feature, fontsize=10)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realmente tem que verificar como proceder nessas variaveis, que acredito que podem ajudar na acuracia do modelo.\n",
    "Vamos criar algumas variaveis multiplicando a media de venda pela media de compra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de Treino\n",
    "dfSample['var_lag3'] = dfSample['avg_sales_lag3'] * dfSample['avg_purchases_lag3']\n",
    "dfSample['var_lag6'] = dfSample['avg_sales_lag6'] * dfSample['avg_purchases_lag6']\n",
    "dfSample['var_lag12'] = dfSample['avg_sales_lag12'] * dfSample['avg_purchases_lag12']\n",
    "dfSample['feature_4'] = (dfSample['feature_1'] * dfSample['feature_1'].mean()) + (dfSample['feature_2'] * dfSample['feature_2'].mean()) + (dfSample['feature_3'] * dfSample['feature_3'].mean())\n",
    "dfSample['feature_5'] = (dfSample['month_lag'] * dfSample['purchase_date_month'])\n",
    "dfSample['feature_6'] = (dfSample['avg_sales_lag3'] * dfSample['active_months_lag3'])\n",
    "dfSample['feature_7'] = (dfSample['avg_sales_lag6'] * dfSample['active_months_lag6'])\n",
    "dfSample['feature_8'] = (dfSample['avg_sales_lag12'] * dfSample['active_months_lag12'])\n",
    "dfSample['feature_9'] = (dfSample['month_lag'] * dfSample['active_months_lag3'])\n",
    "dfSample['feature_10'] = (dfSample['month_lag'] * dfSample['active_months_lag6'])\n",
    "dfSample['feature_11'] = (dfSample['month_lag'] * dfSample['active_months_lag12'])\n",
    "dfSample['feature_12'] = (dfSample['most_recent_sales_range'] * dfSample['avg_sales_lag3'])\n",
    "dfSample['feature_13'] = (dfSample['most_recent_sales_range'] * dfSample['avg_sales_lag6'])\n",
    "dfSample['feature_14'] = (dfSample['most_recent_sales_range'] * dfSample['avg_sales_lag12'])\n",
    "dfSample['feature_15'] = (dfSample['feature_1'] * dfSample['category_1_x'] * dfSample['numerical_1'])\n",
    "dfSample['feature_16'] = (dfSample['feature_2'] * dfSample['category_2_x'] * dfSample['numerical_2'])\n",
    "dfSample['feature_17'] = (dfSample['purchase_amount'] / dfSample['state_id_x'])\n",
    "dfSample['feature_18'] = (dfSample['purchase_amount'] / dfSample['subsector_id_x'])\n",
    "dfSample['feature_19'] = (dfSample['purchase_amount'] / dfSample['merchant_category_id_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de Treino\n",
    "dfTest['var_lag3'] = dfTest['avg_sales_lag3'] * dfTest['avg_purchases_lag3']\n",
    "dfTest['var_lag6'] = dfTest['avg_sales_lag6'] * dfTest['avg_purchases_lag6']\n",
    "dfTest['var_lag12'] = dfTest['avg_sales_lag12'] * dfSample['avg_purchases_lag12']\n",
    "dfTest['feature_4'] = (dfTest['feature_1'] * dfTest['feature_1'].mean()) + (dfTest['feature_2'] * dfTest['feature_2'].mean()) + (dfTest['feature_3'] * dfTest['feature_3'].mean())\n",
    "dfTest['feature_5'] = (dfTest['month_lag'] * dfTest['purchase_date_month'])\n",
    "dfTest['feature_6'] = (dfTest['avg_sales_lag3'] * dfTest['active_months_lag3'])\n",
    "dfTest['feature_7'] = (dfTest['avg_sales_lag6'] * dfTest['active_months_lag6'])\n",
    "dfTest['feature_8'] = (dfTest['avg_sales_lag12'] * dfTest['active_months_lag12'])\n",
    "dfTest['feature_9'] = (dfTest['month_lag'] * dfTest['active_months_lag3'])\n",
    "dfTest['feature_10'] = (dfTest['month_lag'] * dfTest['active_months_lag6'])\n",
    "dfTest['feature_11'] = (dfTest['month_lag'] * dfTest['active_months_lag12'])\n",
    "dfTest['feature_12'] = (dfTest['most_recent_sales_range'] * dfTest['avg_sales_lag3'])\n",
    "dfTest['feature_13'] = (dfTest['most_recent_sales_range'] * dfTest['avg_sales_lag6'])\n",
    "dfTest['feature_14'] = (dfTest['most_recent_sales_range'] * dfTest['avg_sales_lag12'])\n",
    "dfTest['feature_15'] = (dfTest['feature_1'] * dfTest['category_1_x'] * dfTest['numerical_1'])\n",
    "dfTest['feature_16'] = (dfTest['feature_2'] * dfTest['category_2_x'] * dfTest['numerical_2'])\n",
    "dfTest['feature_17'] = (dfTest['purchase_amount'] / dfTest['state_id_x'])\n",
    "dfTest['feature_18'] = (dfTest['purchase_amount'] / dfTest['subsector_id_x'])\n",
    "dfTest['feature_19'] = (dfTest['purchase_amount'] / dfTest['merchant_category_id_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos fazer algumas transformacoes nas features criadas (venda x compra) usando _log e _square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de Treino\n",
    "dfSample[\"var_lag3\"] = np.log1p(dfSample[\"var_lag3\"])\n",
    "dfSample[\"var_lag6\"] = np.log1p(dfSample[\"var_lag6\"])\n",
    "dfSample[\"var_lag12\"] = np.log1p(dfSample[\"var_lag12\"])\n",
    "dfSample[\"feature_4\"] = np.log1p(dfSample[\"feature_4\"])\n",
    "dfSample[\"feature_5\"] = np.log1p(dfSample[\"feature_5\"])\n",
    "dfSample[\"feature_6\"] = np.log1p(dfSample[\"feature_6\"])\n",
    "dfSample[\"feature_7\"] = np.log1p(dfSample[\"feature_7\"])\n",
    "dfSample[\"feature_8\"] = np.log1p(dfSample[\"feature_8\"])\n",
    "dfSample[\"feature_9\"] = np.log1p(dfSample[\"feature_9\"])\n",
    "dfSample[\"feature_10\"] = np.log1p(dfSample[\"feature_10\"])\n",
    "dfSample[\"feature_11\"] = np.log1p(dfSample[\"feature_11\"])\n",
    "dfSample[\"feature_12\"] = np.log1p(dfSample[\"feature_12\"])\n",
    "dfSample[\"feature_13\"] = np.log1p(dfSample[\"feature_13\"])\n",
    "dfSample[\"feature_14\"] = np.log1p(dfSample[\"feature_14\"])\n",
    "dfSample[\"feature_15\"] = np.log1p(dfSample[\"feature_15\"])\n",
    "dfSample[\"feature_16\"] = np.log1p(dfSample[\"feature_16\"])\n",
    "dfSample[\"feature_17\"] = np.log1p(dfSample[\"feature_17\"])\n",
    "dfSample[\"feature_18\"] = np.log1p(dfSample[\"feature_18\"])\n",
    "dfSample[\"feature_19\"] = np.log1p(dfSample[\"feature_19\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de Teste\n",
    "dfTest[\"var_lag3\"] = np.log1p(dfTest[\"var_lag3\"])\n",
    "dfTest[\"var_lag6\"] = np.log1p(dfTest[\"var_lag6\"])\n",
    "dfTest[\"var_lag12\"] = np.log1p(dfTest[\"var_lag12\"])\n",
    "dfTest[\"feature_4\"] = np.log1p(dfTest[\"feature_4\"])\n",
    "dfTest[\"feature_5\"] = np.log1p(dfTest[\"feature_5\"])\n",
    "dfTest[\"feature_6\"] = np.log1p(dfTest[\"feature_6\"])\n",
    "dfTest[\"feature_7\"] = np.log1p(dfTest[\"feature_7\"])\n",
    "dfTest[\"feature_8\"] = np.log1p(dfTest[\"feature_8\"])\n",
    "dfTest[\"feature_9\"] = np.log1p(dfTest[\"feature_9\"])\n",
    "dfTest[\"feature_10\"] = np.log1p(dfTest[\"feature_10\"])\n",
    "dfTest[\"feature_11\"] = np.log1p(dfTest[\"feature_11\"])\n",
    "dfTest[\"feature_12\"] = np.log1p(dfTest[\"feature_12\"])\n",
    "dfTest[\"feature_13\"] = np.log1p(dfTest[\"feature_13\"])\n",
    "dfTest[\"feature_14\"] = np.log1p(dfTest[\"feature_14\"])\n",
    "dfTest[\"feature_15\"] = np.log1p(dfTest[\"feature_15\"])\n",
    "dfTest[\"feature_16\"] = np.log1p(dfTest[\"feature_16\"])\n",
    "dfTest[\"feature_17\"] = np.log1p(dfTest[\"feature_17\"])\n",
    "dfTest[\"feature_18\"] = np.log1p(dfTest[\"feature_18\"])\n",
    "dfTest[\"feature_19\"] = np.log1p(dfTest[\"feature_19\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample = dfSample.replace([np.inf, -np.inf], np.nan)\n",
    "dfSample.update(dfSample.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = dfTest.replace([np.inf, -np.inf], np.nan)\n",
    "dfTest.update(dfTest.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample[dfSample.columns.drop('target')].corrwith(dfSample.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize = (30,30))\n",
    "sns.set(font_scale=1.5)\n",
    "sns.heatmap(dfSample.corr(),square = True,cbar=True,annot=True,annot_kws={'size': 10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebe-se que as features criadas ficaram com baixa correlacao com a variavel target. Mas vamos verificar como esta o modelo, qualquer coisa voltamos aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection - Método Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and labels\n",
    "X = dfSample.drop(['target', 'card_id'], axis=1)\n",
    "y = dfSample['target']\n",
    "\n",
    "# Aplicando a mesma escala nos dados\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Padronizando os dados (0 para a média, 1 para o desvio padrão)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Split X into training and validation sets\n",
    "validation_size = 0.3\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "\n",
    "print(\"Fit Model\")\n",
    "model = XGBRegressor(n_jobs=-1,\n",
    "                     random_state = seed,\n",
    "                     learning_rate= 0.1,\n",
    "                     n_estimators= 10,\n",
    "                     max_depth= 10,\n",
    "                     subsample= 0.9,\n",
    "                     colsample_bytree=0.7\n",
    "                     ) \n",
    "model.fit(X_train, Y_train)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Importances\")\n",
    "colsX = dfSample.columns\n",
    "features = dict(zip(dfSample[colsX], model.feature_importances_))\n",
    "features_sorted = sorted(features.items(), key=lambda kv: kv[1], reverse=True)\n",
    "features_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando os Dados para Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse ponto vamos criar mais uma copia, agora do dataset dfSample, para facilitar o retorno ate aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = dfSample#.copy()\n",
    "#all_features = all_features.filter(features_new)\n",
    "all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse é um ponto a verificar, pois foi a unica forma que encontrei para agrupar os registros em um do cardId. Talvez seja necessario outra estrategia, mas nao encontrei ate o momento. Usei a media de cada variavel para o agrupamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.DataFrame(all_features.groupby( ['card_id'] ).mean().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização e Padronização de features numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split features and labels\n",
    "X = all_features.drop(['target'], axis=1)\n",
    "y = all_features['target']\n",
    "\n",
    "# Aplicando a mesma escala nos dados\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Padronizando os dados (0 para a média, 1 para o desvio padrão)\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape, dfTest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dataset dfTest tem uma coluna a mais por conta do CARD_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação e Validação dos Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coloquei somente 2 splits por causa do tempo, para testar mais rapido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cross validation folds\n",
    "kf = KFold(n_splits=2, random_state=123, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defini a metrica de validacao (RMSL)\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def cv_rmse(model, X=X):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "    return (rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa parte adicionei varios modelos para comparacao. A principio utilizei alguns parametros default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light Gradient Boosting Regressor\n",
    "lightgbm = LGBMRegressor(objective='regression', \n",
    "                       num_leaves=6,\n",
    "                       learning_rate=0.01, \n",
    "                       n_estimators=3000,\n",
    "                       max_bin=200, \n",
    "                       bagging_fraction=0.8,\n",
    "                       bagging_freq=4, \n",
    "                       bagging_seed=8,\n",
    "                       feature_fraction=0.2,\n",
    "                       feature_fraction_seed=8,\n",
    "                       min_sum_hessian_in_leaf = 11,\n",
    "                       #verbose=0,\n",
    "                       random_state=123)\n",
    "\n",
    "# XGBoost Regressor\n",
    "xgboost = XGBRegressor(learning_rate=0.01,\n",
    "                       n_estimators=4000,\n",
    "                       max_depth=4,\n",
    "                       min_child_weight=0,\n",
    "                       gamma=0.6,\n",
    "                       subsample=0.7,\n",
    "                       colsample_bytree=0.7,\n",
    "                       objective='reg:squarederror',\n",
    "                       nthread=-1,\n",
    "                       scale_pos_weight=1,\n",
    "                       seed=27,\n",
    "                       reg_alpha=0.00006,\n",
    "                       #verbosity=3,\n",
    "                       random_state=123)\n",
    "\n",
    "# Support Vector Regressor\n",
    "svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=2000,\n",
    "                                learning_rate=0.01,\n",
    "                                max_depth=4,\n",
    "                                max_features='sqrt',\n",
    "                                min_samples_leaf=15,\n",
    "                                min_samples_split=10,\n",
    "                                loss='huber',\n",
    "                                #verbose=True,\n",
    "                                random_state=123)  \n",
    "\n",
    "# Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=2000,\n",
    "                          max_depth=15,\n",
    "                          min_samples_split=5,\n",
    "                          min_samples_leaf=5,\n",
    "                          max_features=None,\n",
    "                          oob_score=True,\n",
    "                          #verbose=True,\n",
    "                          random_state=123)\n",
    "\n",
    "# KerasRegressor\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=54, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "keras = KerasRegressor(build_fn=baseline_model, \n",
    "                       epochs=100, \n",
    "                       batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "\n",
    "score = cv_rmse(lightgbm)\n",
    "print(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['lgb'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(xgboost)\n",
    "print(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['xgb'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(svr)\n",
    "print(\"svr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['svr'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(rf)\n",
    "print(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['rf'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(gbr)\n",
    "print(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['gbr'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score = cv_rmse(keras)\n",
    "#print(\"keras: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "#scores['kr_norm'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lightgbm')\n",
    "lgb_model_full_data = lightgbm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('xgboost')\n",
    "xgb_model_full_data = xgboost.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Svr')\n",
    "svr_model_full_data = svr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RandomForest')\n",
    "rf_model_full_data = rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GradientBoosting')\n",
    "gbr_model_full_data = gbr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('KerasRegressor')\n",
    "#keras_model_full_data = keras.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blend models and get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo as previsoes finais\n",
    "# Nao consegui colocar o keras pois ele grava um History (estudando como fazer)\n",
    "def blended_predictions(X):\n",
    "    return ((svr_model_full_data.predict(X)) + \\\n",
    "            (gbr_model_full_data.predict(X)) + \\\n",
    "            (xgb_model_full_data.predict(X)) + \\\n",
    "            (lgb_model_full_data.predict(X)) + \\\n",
    "            (rf_model_full_data.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando as predictions dos modelos\n",
    "blended_score = rmsle(y, blended_predictions(X))\n",
    "scores['blended'] = (blended_score, 0)\n",
    "print('RMSLE score no dataset de Treino:')\n",
    "print(blended_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando Gradiente Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingRegressor Otimizado\n",
    "\n",
    "def fit_predict(model, X, y, X_test):\n",
    "    model.fit(X, y)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "def rmspe(y_test, y_pred):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmspe = sqrt(mse)   \n",
    "    return rmspe\n",
    "\n",
    "# Split X into training and validation sets\n",
    "validation_size = 0.10\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "# Melhores parameters\n",
    "gbr_otm = GradientBoostingRegressor(subsample=1, \n",
    "                                  n_estimators=200, \n",
    "                                  max_features='sqrt',\n",
    "                                  max_depth=5, \n",
    "                                  loss='huber', \n",
    "                                  learning_rate=0.1)\n",
    "\n",
    "modelo_gbr_otm = fit_predict(gbr_otm, X_train, Y_train, X_test)\n",
    "gbr_otm = rmsle(Y_test, modelo_gbr_otm)\n",
    "\n",
    "print('GradientBoostingRegressor - Otimizado = %0.4f' % gbr_otm)\n",
    "\n",
    "scores['gbr_otm'] = (gbr_otm.mean(), gbr_otm.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando a melhor performance dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot com a previsao de cada modelo\n",
    "sns.set_style(\"white\")\n",
    "fig = plt.figure(figsize=(24, 12))\n",
    "\n",
    "ax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\n",
    "for i, score in enumerate(scores.values()):\n",
    "    ax.text(i, score[0] , '{:.6f}'.format(score[0]), horizontalalignment='left', size='22', color='black', weight='semibold')\n",
    "\n",
    "plt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\n",
    "plt.xlabel('Model', size=20, labelpad=12.5)\n",
    "plt.tick_params(axis='x', labelsize=13.5)\n",
    "plt.tick_params(axis='y', labelsize=12.5)\n",
    "\n",
    "plt.title('Scores of Models', size=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse ponto da pra verificar que juntando os modelos tivemos melhor performance... mas aqui é o momento de usar o GridSearch para obter os melhores parametros e otimizar o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora fazer previsoes nos dados de teste e visualizar no plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando o split para separar dados de treino e dados de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 10)\n",
    "predictions = blended_predictions(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "fig = plt.figure(figsize=(24, 12))\n",
    "\n",
    "plt.plot(range(y_test.shape[0]),y_test,label=\"Dados Originais\")\n",
    "plt.plot(range(y_test.shape[0]),predictions,label=\"Dados Previstos\")\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('target')\n",
    "plt.title('Comparacao com dados de teste')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_final = pd.DataFrame(dfTest.groupby( ['card_id'] ).mean().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_final = sub_final.replace([np.inf, -np.inf], np.nan)\n",
    "sub_final.update(sub_final.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a mesma escala nos dados\n",
    "X_final = MinMaxScaler().fit_transform(sub_final)\n",
    "\n",
    "# Padronizando os dados (0 para a média, 1 para o desvio padrão)\n",
    "X_final = StandardScaler().fit_transform(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = blended_predictions(X_final)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerando Arquivo de Submissao\n",
    "submission = pd.DataFrame({\n",
    "    \"card_id\": sub_final.index, \n",
    "    \"target\": predictions\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
